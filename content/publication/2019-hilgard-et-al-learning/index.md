---
abstract: We propose a new, complementary approach to interpretability, in which machines are not considered as experts whose role it is to suggest what should be done and why, but rather as advisers. The objective of these models is to communicate to a human decision-maker not what to decide but how to decide. In this way, we propose that machine learning pipelines will be more readily adopted, since they allow a decision-maker to retain agency. Specifically, we develop a framework for learning representations by humans, for humans, in which we learn representations of inputs ('advice') that are effective for human decision-making. Representation-generating models are trained with humans-in-the-loop, implicitly incorporating the human decision-making model. We show that optimizing for human decision-making rather than accuracy is effective in promoting good decisions in various classification tasks while inherently maintaining a sense of interpretability.

authors:
- Sophie Hilgard
- Nir Halevy
- Mahzarin Banaji
- admin
- David Parkes

date: "2019-05-29T00:00:00Z"

doi:

featured: false

image:
  caption: 'Image credit: [**Unsplash**](https://unsplash.com/photos/s9CC2SKySJM)'
  focal_point: ""
  preview_only: true
  
publication: "*ArXiv*"

publication_short: ""

publication_types:
- "3"

publishDate: "2019-05-01T00:00:00Z"

title: Learning Representations by Humans, for Humans
url_code: ''
url_dataset: ''
url_pdf: papers/2019-hilgard-et-al-learning.pdf
url_poster: ''
url_project: ""
url_slides: ""
url_source: ''
url_video: ''
---

<!--
{{% alert note %}}
Click the *Slides* button above to demo Academic's Markdown slides feature.
{{% /alert %}}

Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/).
-->
